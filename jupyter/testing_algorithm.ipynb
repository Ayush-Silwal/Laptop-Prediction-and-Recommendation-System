{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbf77b0-685a-45e5-a12f-42539d21f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Dataset shape after preprocessing: (1302, 18)\n",
      "Price range: $9270.72 - $324954.72\n",
      "Training set: 906 samples\n",
      "Validation set: 200 samples\n",
      "Test set: 196 samples\n",
      "Feature dimensions after preprocessing: 48\n",
      "\n",
      "Training enhanced Random Forest...\n",
      "\n",
      "Random Forest Performance:\n",
      "Validation - MSE: 0.1076, MAE: 0.2583, R²: 0.7178\n",
      "Test - MSE: 0.1125, MAE: 0.2676, R²: 0.6927\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                    feature  importance\n",
      "7             price_per_ram    0.082344\n",
      "9                 ssd_ratio    0.072143\n",
      "4                       ppi    0.070447\n",
      "1                    Weight    0.066750\n",
      "0                       Ram    0.058035\n",
      "6                       SSD    0.056005\n",
      "8             storage_total    0.039717\n",
      "34        TypeName_Notebook    0.032712\n",
      "40  Cpu brand_Intel Core i7    0.031299\n",
      "39  Cpu brand_Intel Core i5    0.030330\n",
      "\n",
      "Training enhanced KNN...\n",
      "Feature weights set based on Random Forest importance\n",
      "\n",
      "KNN Performance:\n",
      "Validation - MSE: 0.0281, MAE: 0.1153, R²: 0.9263\n",
      "Test - MSE: 0.0285, MAE: 0.1167, R²: 0.9223\n",
      "\n",
      "Training enhanced K-Means...\n",
      "\n",
      "K-Means Performance:\n",
      "Silhouette Score: 0.1536\n",
      "Inertia: 5234.10\n",
      "\n",
      "Cluster Analysis:\n",
      "Cluster 0 (Budget-Friendly Everyday Laptops): 107 laptops, Avg Price: $104511, Avg RAM: 17.8GB, Avg Weight: 3.1kg, Common Type: Gaming\n",
      "Cluster 1 (Mid-Range Professional Laptops): 76 laptops, Avg Price: $104184, Avg RAM: 12.9GB, Avg Weight: 1.6kg, Common Type: Ultrabook\n",
      "Cluster 2 (Premium Business Workstations): 281 laptops, Avg Price: $69989, Avg RAM: 7.7GB, Avg Weight: 1.6kg, Common Type: Notebook\n",
      "Cluster 3 (Gaming & High-Performance Systems): 197 laptops, Avg Price: $48506, Avg RAM: 7.5GB, Avg Weight: 2.4kg, Common Type: Notebook\n",
      "Cluster 4 (Ultraportable & Designer Laptops): 245 laptops, Avg Price: $25953, Avg RAM: 4.5GB, Avg Weight: 2.0kg, Common Type: Notebook\n",
      "\n",
      "Creating ensemble predictions...\n",
      "Ensemble Validation - R²: 0.8300, MAE: 0.1945\n",
      "Ensemble Test - R²: 0.8072, MAE: 0.2107\n",
      "\n",
      "Saving enhanced models...\n",
      "✅ Enhanced models saved successfully!\n",
      "📊 Final Model Performance Summary:\n",
      "   Random Forest R²: 0.6927\n",
      "   KNN R²: 0.9223\n",
      "   Ensemble R²: 0.8072\n",
      "   K-Means Silhouette: 0.1536\n",
      "\n",
      "Generating feature configuration for app...\n",
      "💾 Feature configuration saved to feature_config.json\n",
      "\n",
      "🎉 Training completed successfully!\n",
      "📁 Files generated:\n",
      "   - laptop_models_full_custom.pkl (main model file)\n",
      "   - feature_config.json (feature weights for app)\n",
      "\n",
      "You can now use these models in your Flask app with improved accuracy!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, silhouette_score\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import issparse\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================== ENHANCED CUSTOM MODELS ======================\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=5, min_samples_leaf=2, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.tree_ = None\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.values\n",
    "            \n",
    "        n_features = X.shape[1]\n",
    "        if self.max_features is None:\n",
    "            self.max_features = n_features\n",
    "        elif self.max_features == 'sqrt':\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            self.max_features = int(np.log2(n_features))\n",
    "            \n",
    "        self.feature_importances_ = np.zeros(n_features)\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "        self.feature_importances_ /= self.feature_importances_.sum()\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, n_features = X.shape\n",
    "        \n",
    "        # Enhanced stopping conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           num_samples < self.min_samples_split or \\\n",
    "           num_samples < 2 * self.min_samples_leaf or \\\n",
    "           len(np.unique(y)) == 1 or \\\n",
    "           np.var(y) < 1e-7:\n",
    "            return np.mean(y)\n",
    "\n",
    "        # Feature sampling for better generalization\n",
    "        if self.max_features < n_features:\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            feature_indices = np.arange(n_features)\n",
    "\n",
    "        best_split = self._find_best_split(X, y, feature_indices)\n",
    "        if best_split is None:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        # Update feature importance\n",
    "        self.feature_importances_[best_split['feature']] += best_split['importance']\n",
    "        \n",
    "        left_indices = X[:, best_split['feature']] <= best_split['value']\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        # Ensure minimum samples in each leaf\n",
    "        if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {\n",
    "            'feature': best_split['feature'],\n",
    "            'value': best_split['value'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "    \n",
    "    def _find_best_split(self, X, y, feature_indices):\n",
    "        best_split = None\n",
    "        best_score = float('inf')\n",
    "        current_mse = np.var(y)\n",
    "\n",
    "        for feature in feature_indices:\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            if len(unique_values) == 1:\n",
    "                continue\n",
    "                \n",
    "            # Smarter split point selection\n",
    "            if len(unique_values) > 20:\n",
    "                # Use quantiles for continuous features\n",
    "                split_points = np.percentile(unique_values, [10, 25, 50, 75, 90])\n",
    "            else:\n",
    "                # Use midpoints for discrete features\n",
    "                split_points = [(unique_values[i] + unique_values[i+1]) / 2 \n",
    "                               for i in range(len(unique_values)-1)]\n",
    "            \n",
    "            for value in split_points:\n",
    "                left_indices = X[:, feature] <= value\n",
    "                right_indices = ~left_indices\n",
    "                \n",
    "                if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                left_y = y[left_indices]\n",
    "                right_y = y[right_indices]\n",
    "                \n",
    "                # Weighted MSE\n",
    "                left_weight = len(left_y) / len(y)\n",
    "                right_weight = len(right_y) / len(y)\n",
    "                weighted_mse = left_weight * np.var(left_y) + right_weight * np.var(right_y)\n",
    "                \n",
    "                if weighted_mse < best_score:\n",
    "                    importance = current_mse - weighted_mse  # Information gain\n",
    "                    best_split = {\n",
    "                        'feature': feature, \n",
    "                        'value': value, \n",
    "                        'importance': importance\n",
    "                    }\n",
    "                    best_score = weighted_mse\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        return np.array([self._predict(sample, self.tree_) for sample in X])\n",
    "    \n",
    "    def _predict(self, sample, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        \n",
    "        if sample[tree['feature']] <= tree['value']:\n",
    "            return self._predict(sample, tree['left'])\n",
    "        return self._predict(sample, tree['right'])\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=150, max_depth=15, max_features='sqrt', \n",
    "                 min_samples_split=5, min_samples_leaf=2, bootstrap=True, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.feature_indices = []\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.values\n",
    "            \n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Better max_features calculation\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_feats = max(1, int(np.sqrt(n_features)))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_feats = max(1, int(np.log2(n_features)))\n",
    "        elif isinstance(self.max_features, float):\n",
    "            max_feats = max(1, int(self.max_features * n_features))\n",
    "        else:\n",
    "            max_feats = self.max_features or n_features\n",
    "            \n",
    "        feature_importance_sum = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            if self.bootstrap:\n",
    "                indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                X_sample, y_sample = X[indices], y[indices]\n",
    "            else:\n",
    "                X_sample, y_sample = X, y\n",
    "                \n",
    "            # Random feature selection\n",
    "            feature_idx = np.random.choice(n_features, max_feats, replace=False)\n",
    "            \n",
    "            # Create and train tree\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=max_feats\n",
    "            )\n",
    "            \n",
    "            X_sub = X_sample[:, feature_idx]\n",
    "            tree.fit(X_sub, y_sample)\n",
    "            \n",
    "            self.trees.append(tree)\n",
    "            self.feature_indices.append(feature_idx)\n",
    "            \n",
    "            # Accumulate feature importance\n",
    "            if tree.feature_importances_ is not None:\n",
    "                feature_importance_sum[feature_idx] += tree.feature_importances_\n",
    "            \n",
    "        # Normalize feature importance\n",
    "        self.feature_importances_ = feature_importance_sum / self.n_estimators\n",
    "        if self.feature_importances_.sum() > 0:\n",
    "            self.feature_importances_ = self.feature_importances_ / self.feature_importances_.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        all_preds = np.zeros((self.n_estimators, X.shape[0]))\n",
    "        \n",
    "        for i, (tree, feat_idx) in enumerate(zip(self.trees, self.feature_indices)):\n",
    "            X_sub = X[:, feat_idx]\n",
    "            all_preds[i] = tree.predict(X_sub)\n",
    "            \n",
    "        return np.mean(all_preds, axis=0)\n",
    "\n",
    "\n",
    "class CustomKNN:\n",
    "    def __init__(self, k=7, metric='hybrid', weights='distance'):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.weights = weights\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.feature_weights = None\n",
    "        \n",
    "    def set_feature_weights(self, weights):\n",
    "        \"\"\"Set importance weights for different features\"\"\"\n",
    "        self.feature_weights = np.array(weights) if weights is not None else None\n",
    "    \n",
    "    def _cosine_similarity(self, a, b, weights=None):\n",
    "        if weights is not None:\n",
    "            a = a * weights\n",
    "            b = b * weights\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0\n",
    "        return np.dot(a, b) / (norm_a * norm_b)\n",
    "    \n",
    "    def _euclidean_distance(self, a, b, weights=None):\n",
    "        if weights is not None:\n",
    "            a = a * weights\n",
    "            b = b * weights\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "    \n",
    "    def _manhattan_distance(self, a, b, weights=None):\n",
    "        if weights is not None:\n",
    "            a = a * weights\n",
    "            b = b * weights\n",
    "        return np.sum(np.abs(a - b))\n",
    "    \n",
    "    def _hybrid_similarity(self, a, b, weights=None):\n",
    "        \"\"\"Combines multiple similarity metrics for better accuracy\"\"\"\n",
    "        cos_sim = self._cosine_similarity(a, b, weights)\n",
    "        euc_dist = self._euclidean_distance(a, b, weights)\n",
    "        # Convert euclidean to similarity\n",
    "        euc_sim = 1 / (1 + euc_dist)\n",
    "        \n",
    "        # Weighted combination - cosine is better for high-dimensional sparse data\n",
    "        return 0.7 * cos_sim + 0.3 * euc_sim\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        self.X_train = X\n",
    "        self.y_train = y.values if isinstance(y, pd.Series) else y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            if self.metric == 'hybrid':\n",
    "                similarities = [self._hybrid_similarity(sample, x, self.feature_weights) for x in self.X_train]\n",
    "                neighbor_indices = np.argpartition(similarities, -self.k)[-self.k:]\n",
    "                neighbor_similarities = [similarities[i] for i in neighbor_indices]\n",
    "            elif self.metric == 'cosine':\n",
    "                similarities = [self._cosine_similarity(sample, x, self.feature_weights) for x in self.X_train]\n",
    "                neighbor_indices = np.argpartition(similarities, -self.k)[-self.k:]\n",
    "                neighbor_similarities = [similarities[i] for i in neighbor_indices]\n",
    "            else:  # euclidean or manhattan\n",
    "                if self.metric == 'manhattan':\n",
    "                    distances = [self._manhattan_distance(sample, x, self.feature_weights) for x in self.X_train]\n",
    "                else:\n",
    "                    distances = [self._euclidean_distance(sample, x, self.feature_weights) for x in self.X_train]\n",
    "                neighbor_indices = np.argpartition(distances, self.k)[:self.k]\n",
    "                neighbor_similarities = [1/(1+distances[i]) for i in neighbor_indices]  # Convert to similarities\n",
    "            \n",
    "            neighbor_values = self.y_train[neighbor_indices]\n",
    "            \n",
    "            # Apply distance weighting if requested\n",
    "            if self.weights == 'distance':\n",
    "                total_similarity = sum(neighbor_similarities)\n",
    "                if total_similarity > 0:\n",
    "                    weights_array = np.array(neighbor_similarities) / total_similarity\n",
    "                    prediction = np.average(neighbor_values, weights=weights_array)\n",
    "                else:\n",
    "                    prediction = np.mean(neighbor_values)\n",
    "            else:\n",
    "                prediction = np.mean(neighbor_values)\n",
    "                \n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def get_similar_laptops(self, X_input, df, top_n=5, price_range_factor=0.3):\n",
    "        \"\"\"Enhanced similarity search with price filtering and diversity\"\"\"\n",
    "        if isinstance(X_input, pd.DataFrame):\n",
    "            X_input = X_input.values\n",
    "        if issparse(X_input):\n",
    "            X_input = X_input.toarray()\n",
    "            \n",
    "        similarities = []\n",
    "        for i, sample in enumerate(self.X_train):\n",
    "            if self.metric == 'hybrid':\n",
    "                sim = self._hybrid_similarity(X_input[0], sample, self.feature_weights)\n",
    "            else:\n",
    "                sim = self._cosine_similarity(X_input[0], sample, self.feature_weights)\n",
    "            similarities.append((sim, i))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(reverse=True)\n",
    "        \n",
    "        # Get diverse recommendations\n",
    "        recommendations = []\n",
    "        seen_companies = set()\n",
    "        seen_types = set()\n",
    "        \n",
    "        for sim_score, idx in similarities:\n",
    "            if len(recommendations) >= top_n:\n",
    "                break\n",
    "                \n",
    "            laptop = df.iloc[idx].copy()\n",
    "            company = laptop.get('Company', 'Unknown')\n",
    "            type_name = laptop.get('TypeName', 'Unknown')\n",
    "            \n",
    "            # Diversity constraints\n",
    "            company_count = sum(1 for r in recommendations if r.get('Company') == company)\n",
    "            type_key = f\"{company}-{type_name}\"\n",
    "            \n",
    "            if company_count >= 2 or type_key in seen_types:\n",
    "                continue\n",
    "                \n",
    "            seen_companies.add(company)\n",
    "            seen_types.add(type_key)\n",
    "            \n",
    "            # Build comprehensive laptop info\n",
    "            ram = laptop.get('Ram', 0)\n",
    "            ssd = laptop.get('SSD', 0)\n",
    "            hdd = laptop.get('HDD', 0)\n",
    "            cpu = laptop.get('Cpu brand', 'Unknown')\n",
    "            gpu = laptop.get('Gpu brand', 'Unknown')\n",
    "            weight = laptop.get('Weight', 0)\n",
    "            price = laptop.get('Price', 0)\n",
    "            \n",
    "            # Storage description\n",
    "            storage_parts = []\n",
    "            if ssd > 0:\n",
    "                storage_parts.append(f\"{int(ssd)}GB SSD\")\n",
    "            if hdd > 0:\n",
    "                storage_parts.append(f\"{int(hdd)}GB HDD\")\n",
    "            storage = \" + \".join(storage_parts) if storage_parts else \"Storage info unavailable\"\n",
    "            \n",
    "            # Feature classification\n",
    "            features = []\n",
    "            if laptop.get('Touchscreen', 0):\n",
    "                features.append('Touchscreen')\n",
    "            if laptop.get('Ips', 0):\n",
    "                features.append('IPS Display')\n",
    "            \n",
    "            # Performance classification\n",
    "            if ram >= 16 and ssd >= 512:\n",
    "                features.append('High Performance')\n",
    "            elif ram >= 8 and ssd >= 256:\n",
    "                features.append('Mid Performance')\n",
    "            else:\n",
    "                features.append('Basic Performance')\n",
    "            \n",
    "            laptop_info = {\n",
    "                'Company': company,\n",
    "                'TypeName': type_name,\n",
    "                'Title': f\"{company} {type_name}\",\n",
    "                'Ram': f\"{int(ram)}GB\",\n",
    "                'Storage': storage,\n",
    "                'Cpu_brand': cpu,\n",
    "                'Gpu_brand': gpu,\n",
    "                'Weight': f\"{weight:.1f}kg\" if weight > 0 else \"Weight N/A\",\n",
    "                'Price': float(price),\n",
    "                'Similarity': f\"{sim_score:.3f}\",\n",
    "                'Features': ', '.join(features) if features else 'Standard Features',\n",
    "                'Touchscreen': 'Yes' if laptop.get('Touchscreen', 0) else 'No',\n",
    "                'Ips': 'Yes' if laptop.get('Ips', 0) else 'No',\n",
    "                'os': laptop.get('os', 'Unknown OS')\n",
    "            }\n",
    "            \n",
    "            recommendations.append(laptop_info)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "class CustomKMeans:\n",
    "    def __init__(self, n_clusters=5, max_iters=300, random_state=None, init='k-means++'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "        self.init = init\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "        self.inertia_ = None\n",
    "        \n",
    "        # Enhanced cluster names based on laptop characteristics\n",
    "        self.cluster_names = {\n",
    "            0: \"Budget-Friendly Everyday Laptops\",\n",
    "            1: \"Mid-Range Professional Laptops\", \n",
    "            2: \"Premium Business Workstations\",\n",
    "            3: \"Gaming & High-Performance Systems\",\n",
    "            4: \"Ultraportable & Designer Laptops\"\n",
    "        }\n",
    "        \n",
    "    def _kmeans_plus_plus_init(self, X):\n",
    "        \"\"\"K-means++ initialization for better cluster centroids\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        centroids = np.zeros((self.n_clusters, n_features))\n",
    "        \n",
    "        # Choose first centroid randomly\n",
    "        centroids[0] = X[np.random.choice(n_samples)]\n",
    "        \n",
    "        for k in range(1, self.n_clusters):\n",
    "            # Calculate distances to nearest centroid\n",
    "            distances = np.array([min([np.sum((x - c) ** 2) for c in centroids[:k]]) for x in X])\n",
    "            \n",
    "            # Choose next centroid with probability proportional to squared distance\n",
    "            probabilities = distances / distances.sum()\n",
    "            cumulative_probabilities = probabilities.cumsum()\n",
    "            r = np.random.random()\n",
    "            \n",
    "            for j, p in enumerate(cumulative_probabilities):\n",
    "                if r < p:\n",
    "                    centroids[k] = X[j]\n",
    "                    break\n",
    "                    \n",
    "        return centroids\n",
    "        \n",
    "    def fit(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize centroids\n",
    "        if self.init == 'k-means++':\n",
    "            self.centroids = self._kmeans_plus_plus_init(X)\n",
    "        else:\n",
    "            idx = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "            self.centroids = X[idx]\n",
    "        \n",
    "        prev_inertia = float('inf')\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            # Assign points to nearest centroid\n",
    "            old_labels = self.labels_.copy() if self.labels_ is not None else np.zeros(n_samples)\n",
    "            distances = self._calculate_distances(X)\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Calculate inertia\n",
    "            self.inertia_ = np.sum([distances[i, self.labels_[i]] for i in range(n_samples)])\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.all(old_labels == self.labels_) or abs(prev_inertia - self.inertia_) < 1e-6:\n",
    "                break\n",
    "                \n",
    "            prev_inertia = self.inertia_\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids = np.zeros_like(self.centroids)\n",
    "            for k in range(self.n_clusters):\n",
    "                if np.sum(self.labels_ == k) > 0:\n",
    "                    new_centroids[k] = np.mean(X[self.labels_ == k], axis=0)\n",
    "                else:\n",
    "                    # If cluster is empty, reinitialize\n",
    "                    new_centroids[k] = X[np.random.choice(n_samples)]\n",
    "            \n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calculate_distances(self, X):\n",
    "        \"\"\"Calculate distances from all points to all centroids\"\"\"\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            distances[:, k] = np.sum((X - self.centroids[k]) ** 2, axis=1)\n",
    "        return distances\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        distances = self._calculate_distances(X)\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def get_cluster_examples(self, cluster_id, df, X_all=None, top_n=5):\n",
    "        \"\"\"Get diverse and representative examples from the cluster\"\"\"\n",
    "        try:\n",
    "            if X_all is not None:\n",
    "                cluster_labels = self.predict(X_all)\n",
    "            else:\n",
    "                cluster_labels = self.labels_\n",
    "                \n",
    "            cluster_mask = cluster_labels == cluster_id\n",
    "            cluster_df = df[cluster_mask].copy()\n",
    "            \n",
    "            if len(cluster_df) == 0:\n",
    "                return []\n",
    "            \n",
    "            # Enhanced diversity scoring with more factors\n",
    "            cluster_df['diversity_score'] = (\n",
    "                cluster_df['Ram'] * 0.25 +                    # Memory importance\n",
    "                cluster_df.get('SSD', 0) * 0.0001 +           # SSD storage\n",
    "                cluster_df.get('ppi', 100) * 0.008 +          # Display quality\n",
    "                (cluster_df['Weight'] * -3) +                 # Lighter is better\n",
    "                cluster_df.get('Touchscreen', 0) * 8 +        # Premium features\n",
    "                cluster_df.get('Ips', 0) * 8 +                # Display quality\n",
    "                np.random.normal(0, 2, len(cluster_df))       # Add variety\n",
    "            )\n",
    "            \n",
    "            # Sort by diversity and select varied examples\n",
    "            cluster_df_sorted = cluster_df.sort_values(['diversity_score', 'Price'], \n",
    "                                                      ascending=[False, True])\n",
    "            \n",
    "            examples = []\n",
    "            seen_companies = set()\n",
    "            \n",
    "            for _, laptop in cluster_df_sorted.iterrows():\n",
    "                if len(examples) >= top_n:\n",
    "                    break\n",
    "                    \n",
    "                company = laptop.get('Company', 'Unknown')\n",
    "                \n",
    "                # Ensure brand diversity\n",
    "                if len(seen_companies) < 3 or company not in seen_companies:\n",
    "                    seen_companies.add(company)\n",
    "                    \n",
    "                    type_name = laptop.get('TypeName', 'Laptop')\n",
    "                    ram = laptop.get('Ram', 0)\n",
    "                    ssd = laptop.get('SSD', 0)\n",
    "                    hdd = laptop.get('HDD', 0)\n",
    "                    cpu = laptop.get('Cpu brand', 'Unknown')\n",
    "                    gpu = laptop.get('Gpu brand', 'Unknown')\n",
    "                    weight = laptop.get('Weight', 0)\n",
    "                    price = laptop.get('Price', 0)\n",
    "                    \n",
    "                    # Build storage info\n",
    "                    storage_parts = []\n",
    "                    if ssd > 0:\n",
    "                        storage_parts.append(f\"{int(ssd)}GB SSD\")\n",
    "                    if hdd > 0:\n",
    "                        storage_parts.append(f\"{int(hdd)}GB HDD\")\n",
    "                    storage = \" + \".join(storage_parts) if storage_parts else \"No storage info\"\n",
    "                    \n",
    "                    # Enhanced feature list\n",
    "                    features = []\n",
    "                    if laptop.get('Touchscreen', 0):\n",
    "                        features.append('Touchscreen')\n",
    "                    if laptop.get('Ips', 0):\n",
    "                        features.append('IPS Display')\n",
    "                    if ram >= 16:\n",
    "                        features.append('High Memory')\n",
    "                    if ssd >= 512:\n",
    "                        features.append('Fast Storage')\n",
    "                    if weight < 2.0:\n",
    "                        features.append('Lightweight')\n",
    "                        \n",
    "                    features_text = ', '.join(features) if features else 'Standard Features'\n",
    "                    \n",
    "                    example = {\n",
    "                        'Company': company,\n",
    "                        'TypeName': type_name,\n",
    "                        'Title': f\"{company} {type_name}\",\n",
    "                        'Ram': f\"{int(ram)}GB\",\n",
    "                        'Storage': storage,\n",
    "                        'Cpu_brand': cpu,\n",
    "                        'Gpu_brand': gpu,\n",
    "                        'Weight': f\"{weight:.1f}kg\" if weight > 0 else \"Weight N/A\",\n",
    "                        'Price': f\"₹{price:,.2f}\",  # Fixed currency symbol\n",
    "                        'Features': features_text,\n",
    "                        'Touchscreen': 'Yes' if laptop.get('Touchscreen', 0) else 'No',\n",
    "                        'Ips': 'Yes' if laptop.get('Ips', 0) else 'No',\n",
    "                        'os': laptop.get('os', 'Unknown OS')\n",
    "                    }\n",
    "                    \n",
    "                    examples.append(example)\n",
    "            \n",
    "            return examples\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting cluster examples: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# ====================== ENHANCED DATA PIPELINE ======================\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "df = pd.read_csv('laptop_data.csv')\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "# Data cleaning with better error handling\n",
    "df[\"Ram\"] = df[\"Ram\"].str.replace(\"GB\", \"\").astype(\"int\")\n",
    "df[\"Weight\"] = df[\"Weight\"].str.replace(\"kg\", \"\").astype(\"float\")\n",
    "\n",
    "# Remove outliers\n",
    "df = df[(df['Price'] > 1000) & (df['Price'] < 500000)]  # Reasonable price range\n",
    "df = df[(df['Weight'] > 0.5) & (df['Weight'] < 5.0)]    # Reasonable weight range\n",
    "df = df[(df['Ram'] >= 2) & (df['Ram'] <= 64)]           # Reasonable RAM range\n",
    "\n",
    "# Feature engineering\n",
    "df[\"Touchscreen\"] = df[\"ScreenResolution\"].apply(lambda x: 1 if \"Touchscreen\" in x else 0)\n",
    "df[\"Ips\"] = df[\"ScreenResolution\"].apply(lambda x: 1 if \"IPS\" in x else 0)\n",
    "\n",
    "# Enhanced resolution processing\n",
    "temp = df[\"ScreenResolution\"].str.split(\"x\", n=1, expand=True)\n",
    "df[\"X_res\"] = temp[0].str.replace(',', '').str.findall(r'(\\d+\\.?\\d+)').apply(lambda x: x[0] if x else 1920).astype(int)\n",
    "df[\"Y_res\"] = temp[1].fillna('1080').astype(int)\n",
    "df['ppi'] = (((df['X_res']**2) + (df['Y_res']**2))**0.5/df['Inches']).astype('float')\n",
    "df.drop(columns=[\"ScreenResolution\", \"X_res\", \"Y_res\", \"Inches\"], inplace=True)\n",
    "\n",
    "# Enhanced CPU processing\n",
    "df['Cpu Name'] = df['Cpu'].apply(lambda x: \" \".join(x.split()[0:3]))\n",
    "def fetch_processor(text):\n",
    "    if text in ['Intel Core i7', 'Intel Core i5', 'Intel Core i3']:\n",
    "        return text\n",
    "    elif 'Intel' in text:\n",
    "        return 'Other Intel Processor'\n",
    "    else:\n",
    "        return 'AMD Processor'\n",
    "df['Cpu brand'] = df['Cpu Name'].apply(fetch_processor)\n",
    "df.drop(columns=['Cpu', 'Cpu Name'], inplace=True)\n",
    "\n",
    "# Enhanced Memory processing\n",
    "df['Memory'] = df['Memory'].astype(str).replace(r'\\.0', '', regex=True)\n",
    "df[\"Memory\"] = df[\"Memory\"].str.replace('GB', '').str.replace('TB', '000')\n",
    "new = df[\"Memory\"].str.split(\"+\", n=1, expand=True)\n",
    "\n",
    "# Better memory parsing\n",
    "df[\"first\"] = new[0].str.strip()\n",
    "df[\"second\"] = new[1].fillna(\"0\").str.strip()\n",
    "\n",
    "# Extract numbers and storage types\n",
    "def extract_storage(storage_str):\n",
    "    if pd.isna(storage_str) or storage_str == \"0\":\n",
    "        return 0, 'None'\n",
    "    \n",
    "    # Extract number\n",
    "    numbers = re.findall(r'\\d+', str(storage_str))\n",
    "    if not numbers:\n",
    "        return 0, 'None'\n",
    "    \n",
    "    size = int(numbers[0])\n",
    "    storage_type = 'HDD' if 'HDD' in str(storage_str) else ('SSD' if 'SSD' in str(storage_str) else 'Flash')\n",
    "    return size, storage_type\n",
    "\n",
    "import re\n",
    "df[['first_size', 'first_type']] = df['first'].apply(lambda x: pd.Series(extract_storage(x)))\n",
    "df[['second_size', 'second_type']] = df['second'].apply(lambda x: pd.Series(extract_storage(x)))\n",
    "\n",
    "# Calculate HDD and SSD\n",
    "df[\"HDD\"] = 0\n",
    "df[\"SSD\"] = 0\n",
    "\n",
    "# First storage device\n",
    "df.loc[df['first_type'] == 'HDD', 'HDD'] += df['first_size']\n",
    "df.loc[df['first_type'].isin(['SSD', 'Flash']), 'SSD'] += df['first_size']\n",
    "\n",
    "# Second storage device\n",
    "df.loc[df['second_type'] == 'HDD', 'HDD'] += df['second_size']\n",
    "df.loc[df['second_type'].isin(['SSD', 'Flash']), 'SSD'] += df['second_size']\n",
    "\n",
    "df.drop(columns=['first', 'second', 'first_size', 'first_type', 'second_size', 'second_type', 'Memory'], inplace=True)\n",
    "\n",
    "# Enhanced GPU processing\n",
    "df['Gpu brand'] = df['Gpu'].apply(lambda x: x.split()[0])\n",
    "df = df[df['Gpu brand'] != 'ARM']  # Remove ARM GPUs\n",
    "df.drop(columns=['Gpu'], inplace=True)\n",
    "\n",
    "# Enhanced OS processing\n",
    "def cat_os(inp):\n",
    "    if inp in ['Windows 10', 'Windows 7', 'Windows 10 S']:\n",
    "        return 'Windows'\n",
    "    elif inp in ['macOS', 'Mac OS X']:\n",
    "        return 'Mac'\n",
    "    else:\n",
    "        return 'Others/No OS/Linux'\n",
    "df['os'] = df['OpSys'].apply(cat_os)\n",
    "df.drop(columns=['OpSys'], inplace=True)\n",
    "\n",
    "# Additional feature engineering\n",
    "df['price_per_ram'] = df['Price'] / df['Ram']\n",
    "df['storage_total'] = df['HDD'] + df['SSD']\n",
    "df['ssd_ratio'] = df['SSD'] / (df['SSD'] + df['HDD'] + 1)  # +1 to avoid division by zero\n",
    "df['is_gaming'] = df['TypeName'].apply(lambda x: 1 if 'Gaming' in str(x) else 0)\n",
    "df['is_ultrabook'] = df['TypeName'].apply(lambda x: 1 if 'Ultrabook' in str(x) else 0)\n",
    "\n",
    "print(f\"Dataset shape after preprocessing: {df.shape}\")\n",
    "print(f\"Price range: ${df['Price'].min():.2f} - ${df['Price'].max():.2f}\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['Price'])\n",
    "y = np.log(df['Price'])  # Log transformation for better distribution\n",
    "\n",
    "# Enhanced preprocessing pipeline with robust scaling\n",
    "cat_cols = ['Company', 'TypeName', 'Cpu brand', 'Gpu brand', 'os']\n",
    "num_cols = ['Ram', 'Weight', 'Touchscreen', 'Ips', 'ppi', 'HDD', 'SSD', \n",
    "            'price_per_ram', 'storage_total', 'ssd_ratio', 'is_gaming', 'is_ultrabook']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', RobustScaler(), num_cols),  # RobustScaler is better for outliers\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "# Train/validation/test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.18, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\") \n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Transform data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "if issparse(X_train_transformed):\n",
    "    X_train_transformed = X_train_transformed.toarray()\n",
    "if issparse(X_val_transformed):\n",
    "    X_val_transformed = X_val_transformed.toarray()\n",
    "if issparse(X_test_transformed):\n",
    "    X_test_transformed = X_test_transformed.toarray()\n",
    "\n",
    "print(f\"Feature dimensions after preprocessing: {X_train_transformed.shape[1]}\")\n",
    "\n",
    "# ====================== ENHANCED MODEL TRAINING ======================\n",
    "\n",
    "print(\"\\nTraining enhanced Random Forest...\")\n",
    "rf_model = RandomForest(\n",
    "    n_estimators=200, \n",
    "    max_depth=20, \n",
    "    max_features='sqrt',\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_rf = rf_model.predict(X_val_transformed)\n",
    "val_mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "val_mae_rf = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "val_r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_rf = rf_model.predict(X_test_transformed)\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(f\"Validation - MSE: {val_mse_rf:.4f}, MAE: {val_mae_rf:.4f}, R²: {val_r2_rf:.4f}\")\n",
    "print(f\"Test - MSE: {test_mse_rf:.4f}, MAE: {test_mae_rf:.4f}, R²: {test_r2_rf:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(rf_model, 'feature_importances_') and rf_model.feature_importances_ is not None:\n",
    "    feature_names = num_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "print(\"\\nTraining enhanced KNN...\")\n",
    "knn_model = CustomKNN(k=7, metric='hybrid', weights='distance')\n",
    "\n",
    "# Set feature weights based on RF importance\n",
    "if hasattr(rf_model, 'feature_importances_') and rf_model.feature_importances_ is not None:\n",
    "    # Normalize importances and boost important features\n",
    "    weights = rf_model.feature_importances_.copy()\n",
    "    weights = weights / weights.max()  # Normalize to [0, 1]\n",
    "    weights = weights * 2 + 0.5  # Scale to [0.5, 2.5] range\n",
    "    knn_model.set_feature_weights(weights)\n",
    "    print(\"Feature weights set based on Random Forest importance\")\n",
    "\n",
    "knn_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate KNN\n",
    "y_val_pred_knn = knn_model.predict(X_val_transformed)\n",
    "val_mse_knn = mean_squared_error(y_val, y_val_pred_knn)\n",
    "val_mae_knn = mean_absolute_error(y_val, y_val_pred_knn)\n",
    "val_r2_knn = r2_score(y_val, y_val_pred_knn)\n",
    "\n",
    "y_test_pred_knn = knn_model.predict(X_test_transformed)\n",
    "test_mse_knn = mean_squared_error(y_test, y_test_pred_knn)\n",
    "test_mae_knn = mean_absolute_error(y_test, y_test_pred_knn)\n",
    "test_r2_knn = r2_score(y_test, y_test_pred_knn)\n",
    "\n",
    "print(\"\\nKNN Performance:\")\n",
    "print(f\"Validation - MSE: {val_mse_knn:.4f}, MAE: {val_mae_knn:.4f}, R²: {val_r2_knn:.4f}\")\n",
    "print(f\"Test - MSE: {test_mse_knn:.4f}, MAE: {test_mae_knn:.4f}, R²: {test_r2_knn:.4f}\")\n",
    "\n",
    "print(\"\\nTraining enhanced K-Means...\")\n",
    "kmeans_model = CustomKMeans(n_clusters=5, max_iters=300, random_state=42, init='k-means++')\n",
    "kmeans_model.fit(X_train_transformed)\n",
    "\n",
    "# Evaluate K-Means\n",
    "train_silhouette = silhouette_score(X_train_transformed, kmeans_model.labels_)\n",
    "print(f\"\\nK-Means Performance:\")\n",
    "print(f\"Silhouette Score: {train_silhouette:.4f}\")\n",
    "print(f\"Inertia: {kmeans_model.inertia_:.2f}\")\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nCluster Analysis:\")\n",
    "train_cluster_labels = kmeans_model.predict(X_train_transformed)\n",
    "for i in range(5):\n",
    "    cluster_mask = train_cluster_labels == i\n",
    "    cluster_data = X_train[cluster_mask]\n",
    "    avg_price = np.exp(y_train[cluster_mask]).mean()  # Convert back from log\n",
    "    avg_ram = cluster_data['Ram'].mean()\n",
    "    avg_weight = cluster_data['Weight'].mean()\n",
    "    common_type = cluster_data['TypeName'].mode().iloc[0] if len(cluster_data) > 0 else 'Unknown'\n",
    "    \n",
    "    print(f\"Cluster {i} ({kmeans_model.cluster_names.get(i, f'Cluster {i}')}): \"\n",
    "          f\"{cluster_mask.sum()} laptops, \"\n",
    "          f\"Avg Price: ${avg_price:.0f}, \"\n",
    "          f\"Avg RAM: {avg_ram:.1f}GB, \"\n",
    "          f\"Avg Weight: {avg_weight:.1f}kg, \"\n",
    "          f\"Common Type: {common_type}\")\n",
    "\n",
    "# ====================== MODEL ENSEMBLE (BONUS) ======================\n",
    "\n",
    "print(\"\\nCreating ensemble predictions...\")\n",
    "# Combine RF and KNN predictions with weights\n",
    "ensemble_val_pred = 0.7 * y_val_pred_rf + 0.3 * y_val_pred_knn\n",
    "ensemble_test_pred = 0.7 * y_test_pred_rf + 0.3 * y_test_pred_knn\n",
    "\n",
    "ensemble_val_r2 = r2_score(y_val, ensemble_val_pred)\n",
    "ensemble_test_r2 = r2_score(y_test, ensemble_test_pred)\n",
    "ensemble_val_mae = mean_absolute_error(y_val, ensemble_val_pred)\n",
    "ensemble_test_mae = mean_absolute_error(y_test, ensemble_test_pred)\n",
    "\n",
    "print(f\"Ensemble Validation - R²: {ensemble_val_r2:.4f}, MAE: {ensemble_val_mae:.4f}\")\n",
    "print(f\"Ensemble Test - R²: {ensemble_test_r2:.4f}, MAE: {ensemble_test_mae:.4f}\")\n",
    "\n",
    "# ====================== ENHANCED MODEL SAVING ======================\n",
    "\n",
    "print(\"\\nSaving enhanced models...\")\n",
    "\n",
    "# Add metadata for better tracking\n",
    "model_metadata = {\n",
    "    'training_date': pd.Timestamp.now().isoformat(),\n",
    "    'dataset_size': len(df),\n",
    "    'feature_count': X_train_transformed.shape[1],\n",
    "    'rf_performance': {\n",
    "        'test_r2': test_r2_rf,\n",
    "        'test_mae': test_mae_rf,\n",
    "        'val_r2': val_r2_rf\n",
    "    },\n",
    "    'knn_performance': {\n",
    "        'test_r2': test_r2_knn,\n",
    "        'test_mae': test_mae_knn,\n",
    "        'val_r2': val_r2_knn\n",
    "    },\n",
    "    'ensemble_performance': {\n",
    "        'test_r2': ensemble_test_r2,\n",
    "        'test_mae': ensemble_test_mae,\n",
    "        'val_r2': ensemble_val_r2\n",
    "    },\n",
    "    'kmeans_performance': {\n",
    "        'silhouette_score': train_silhouette,\n",
    "        'inertia': kmeans_model.inertia_\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump({\n",
    "    'df': df,\n",
    "    'preprocessor': preprocessor,\n",
    "    'random_forest': rf_model,\n",
    "    'knn': knn_model,\n",
    "    'kmeans': kmeans_model,\n",
    "    'metadata': model_metadata,\n",
    "    'feature_names': num_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "}, 'laptop_models_full_custom.pkl')\n",
    "\n",
    "print(\"✅ Enhanced models saved successfully!\")\n",
    "print(f\"📊 Final Model Performance Summary:\")\n",
    "print(f\"   Random Forest R²: {test_r2_rf:.4f}\")\n",
    "print(f\"   KNN R²: {test_r2_knn:.4f}\")\n",
    "print(f\"   Ensemble R²: {ensemble_test_r2:.4f}\")\n",
    "print(f\"   K-Means Silhouette: {train_silhouette:.4f}\")\n",
    "\n",
    "# ====================== SAVE FEATURE WEIGHTS FOR APP ======================\n",
    "\n",
    "print(\"\\nGenerating feature configuration for app...\")\n",
    "if hasattr(rf_model, 'feature_importances_') and rf_model.feature_importances_ is not None:\n",
    "    # Create feature weight mapping for app.py\n",
    "    feature_config = {\n",
    "        'weights': rf_model.feature_importances_.tolist(),\n",
    "        'feature_count': len(rf_model.feature_importances_),\n",
    "        'top_features': importance_df.head(10).to_dict('records')\n",
    "    }\n",
    "    \n",
    "    with open('feature_config.json', 'w') as f:\n",
    "        import json\n",
    "        json.dump(feature_config, f, indent=2)\n",
    "    \n",
    "    print(\"💾 Feature configuration saved to feature_config.json\")\n",
    "\n",
    "print(\"\\n🎉 Training completed successfully!\")\n",
    "print(\"📁 Files generated:\")\n",
    "print(\"   - laptop_models_full_custom.pkl (main model file)\")\n",
    "print(\"   - feature_config.json (feature weights for app)\")\n",
    "print(\"\\nYou can now use these models in your Flask app with improved accuracy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92263e53-3cfe-4840-a91e-c364368e8a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:38:16,426 - INFO - Loading and preprocessing data...\n",
      "2025-09-22 00:38:16,476 - INFO - Target (y) range: min=9.1346, max=12.0304, mean=10.8126\n",
      "2025-09-22 00:38:16,502 - INFO - PCA explained variance ratio: 1.0000\n",
      "2025-09-22 00:38:16,503 - INFO - Tuning Random Forest...\n",
      "2025-09-22 00:38:16,517 - INFO - Training Random Forest...\n",
      "2025-09-22 00:38:47,487 - INFO - Random Forest predictions sample: [-0.47391216 -0.47391216 -0.47391216 -0.47391216 -0.47391216]\n",
      "2025-09-22 00:38:47,487 - INFO - R² for n_estimators=100, max_depth=10, min_samples_split=10: -336.5811\n",
      "2025-09-22 00:38:47,487 - INFO - Training Random Forest...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, silhouette_score\n",
    "from sklearn.ensemble import RandomForestRegressor  # Fallback\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ====================== CUSTOM MODELS ======================\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=10, min_impurity_decrease=0.001):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.tree_ = None\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.values\n",
    "        self.feature_importances_ = np.zeros(X.shape[1])\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "        self.feature_importances_ /= np.sum(self.feature_importances_) + 1e-10\n",
    "        return self\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples = X.shape[0]\n",
    "        \n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           num_samples < self.min_samples_split or \\\n",
    "           len(np.unique(y)) == 1:\n",
    "            return {'value': np.mean(y), 'size': num_samples}\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None or best_split['gain'] < self.min_impurity_decrease:\n",
    "            return {'value': np.mean(y), 'size': num_samples}\n",
    "        \n",
    "        left_indices = X[:, best_split['feature']] <= best_split['value']\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        if np.sum(left_indices) < 2 or np.sum(right_indices) < 2:\n",
    "            return {'value': np.mean(y), 'size': num_samples}\n",
    "        \n",
    "        self.feature_importances_[best_split['feature']] += best_split['gain'] * num_samples\n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {\n",
    "            'feature': best_split['feature'],\n",
    "            'value': best_split['value'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree,\n",
    "            'size': num_samples\n",
    "        }\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_mse = float('inf')\n",
    "        best_gain = 0\n",
    "        num_features = X.shape[1]\n",
    "        total_var = np.var(y) * len(y)\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            unique_values = np.unique(X[:, feature])\n",
    "            split_points = np.percentile(unique_values, [25, 50, 75]) if len(unique_values) > 10 else unique_values\n",
    "            \n",
    "            for value in split_points:\n",
    "                left_indices = X[:, feature] <= value\n",
    "                right_indices = ~left_indices\n",
    "                \n",
    "                if np.sum(left_indices) < 2 or np.sum(right_indices) < 2:\n",
    "                    continue\n",
    "                \n",
    "                left_y = y[left_indices]\n",
    "                right_y = y[right_indices]\n",
    "                \n",
    "                mse = (np.var(left_y) * len(left_y) + np.var(right_y) * len(right_y)) / len(y)\n",
    "                gain = total_var - mse\n",
    "                \n",
    "                if mse < best_mse:\n",
    "                    best_split = {'feature': feature, 'value': value, 'gain': gain}\n",
    "                    best_mse = mse\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        return np.array([self._predict(sample, self.tree_) for sample in X])\n",
    "    \n",
    "    def _predict(self, sample, tree):\n",
    "        if 'value' in tree:\n",
    "            return tree['value']\n",
    "        if sample[tree['feature']] <= tree['value']:\n",
    "            return self._predict(sample, tree['left'])\n",
    "        return self._predict(sample, tree['right'])\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        logging.info(\"Training Random Forest...\")\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.values\n",
    "            \n",
    "        n_features = X.shape[1]\n",
    "        self.feature_importances_ = np.zeros(n_features)\n",
    "        max_feats = int(np.sqrt(n_features)) if self.max_features == 'sqrt' else self.max_features\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = resample(X, y)\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=10, min_impurity_decrease=0.001)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "            self.feature_importances_ += tree.feature_importances_\n",
    "        \n",
    "        self.feature_importances_ /= self.n_estimators + 1e-10\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        all_preds = np.zeros((self.n_estimators, X.shape[0]))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            all_preds[i] = tree.predict(X)\n",
    "        predictions = np.mean(all_preds, axis=0)\n",
    "        logging.info(f\"Random Forest predictions sample: {predictions[:5]}\")\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class CustomKNN:\n",
    "    def __init__(self, k=5, metric='cosine'):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def _cosine_similarity(self, a, b):\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0\n",
    "        return np.dot(a, b) / (norm_a * norm_b)\n",
    "    \n",
    "    def _euclidean_distance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        logging.info(\"Training KNN...\")\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        self.X_train = X\n",
    "        self.y_train = y.values if isinstance(y, pd.Series) else y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            if self.metric == 'cosine':\n",
    "                distances = np.array([self._cosine_similarity(sample, x) for x in self.X_train])\n",
    "                neighbors = np.argpartition(distances, -self.k)[-self.k:]\n",
    "                weights = distances[neighbors]\n",
    "            else:\n",
    "                distances = np.array([self._euclidean_distance(sample, x) for x in self.X_train])\n",
    "                neighbors = np.argpartition(distances, self.k)[:self.k]\n",
    "                weights = 1 / (distances[neighbors] + 1e-10)\n",
    "            \n",
    "            prediction = np.average(self.y_train[neighbors], weights=weights)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def get_similar_laptops(self, X_input, df, top_n=5):\n",
    "        logging.info(\"Generating laptop recommendations...\")\n",
    "        if isinstance(X_input, pd.DataFrame):\n",
    "            X_input = X_input.values\n",
    "        if issparse(X_input):\n",
    "            X_input = X_input.toarray()\n",
    "            \n",
    "        similarities = []\n",
    "        for i, sample in enumerate(self.X_train):\n",
    "            sim = self._cosine_similarity(X_input[0], sample)\n",
    "            similarities.append((sim, i))\n",
    "        \n",
    "        similarities.sort(reverse=True)\n",
    "        top_indices = [idx for _, idx in similarities[:top_n]]\n",
    "        \n",
    "        recommendations = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            laptop = df.iloc[idx].copy()\n",
    "            \n",
    "            company = laptop.get('Company', 'Unknown')\n",
    "            type_name = laptop.get('TypeName', 'Laptop')\n",
    "            ram = laptop.get('Ram', 0)\n",
    "            ssd = laptop.get('SSD', 0)\n",
    "            hdd = laptop.get('HDD', 0)\n",
    "            cpu = laptop.get('Cpu brand', 'Unknown')\n",
    "            gpu = laptop.get('Gpu brand', 'Unknown')\n",
    "            weight = laptop.get('Weight', 0)\n",
    "            price = laptop.get('Price', 0)\n",
    "            \n",
    "            storage_parts = []\n",
    "            if ssd > 0:\n",
    "                storage_parts.append(f\"{ssd}GB SSD\")\n",
    "            if hdd > 0:\n",
    "                storage_parts.append(f\"{hdd}GB HDD\")\n",
    "            storage = \" + \".join(storage_parts) if storage_parts else \"Storage info unavailable\"\n",
    "            \n",
    "            laptop_info = {\n",
    "                'Company': company,\n",
    "                'TypeName': type_name,\n",
    "                'Title': f\"{company} {type_name}\",\n",
    "                'Ram': f\"{ram}GB\",\n",
    "                'Storage': storage,\n",
    "                'Cpu_brand': cpu,\n",
    "                'Gpu_brand': gpu,\n",
    "                'Weight': f\"{weight:.1f}kg\" if weight > 0 else \"Weight N/A\",\n",
    "                'Price': f\"RS {price:,.2f}\",\n",
    "                'Similarity': f\"{similarities[i][0]:.2%}\"\n",
    "            }\n",
    "            \n",
    "            if laptop.get('Touchscreen', 0):\n",
    "                laptop_info['Features'] = ['Touchscreen']\n",
    "            if laptop.get('Ips', 0):\n",
    "                laptop_info['Features'] = laptop_info.get('Features', []) + ['IPS Display']\n",
    "            \n",
    "            laptop_info['Features'] = ', '.join(laptop_info.get('Features', [])) or 'Standard Features'\n",
    "            recommendations.append(laptop_info)\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "class CustomKMeans:\n",
    "    def __init__(self, n_clusters=5, max_iters=100, random_state=None, init='k-means++'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "        self.init = init\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "        self.cluster_names = {\n",
    "            0: \"Budget-Friendly Laptops\",\n",
    "            1: \"Mid-Range Performance\",\n",
    "            2: \"Premium Workstations\",\n",
    "            3: \"Gaming & High-Performance\",\n",
    "            4: \"Ultraportable & Business\"\n",
    "        }\n",
    "        \n",
    "    def fit(self, X):\n",
    "        logging.info(f\"Training K-Means with {self.n_clusters} clusters...\")\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            \n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        n_samples, n_features = X.shape\n",
    "        if self.init == 'k-means++':\n",
    "            self.centroids = self._kmeans_plus_plus(X)\n",
    "        else:\n",
    "            idx = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "            self.centroids = X[idx]\n",
    "        \n",
    "        for _ in range(self.max_iters):\n",
    "            old_labels = self.labels_ if self.labels_ is not None else np.zeros(n_samples)\n",
    "            self.labels_ = self._assign_clusters(X)\n",
    "            \n",
    "            if np.all(old_labels == self.labels_):\n",
    "                break\n",
    "                \n",
    "            for k in range(self.n_clusters):\n",
    "                if np.sum(self.labels_ == k) > 0:\n",
    "                    self.centroids[k] = np.mean(X[self.labels_ == k], axis=0)\n",
    "                else:\n",
    "                    self.centroids[k] = X[np.random.choice(n_samples)]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _kmeans_plus_plus(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        centroids = [X[np.random.choice(n_samples)]]\n",
    "        \n",
    "        for _ in range(1, self.n_clusters):\n",
    "            distances = np.array([min([np.sum((x - c) ** 2) for c in centroids]) for x in X])\n",
    "            probs = distances / (distances.sum() + 1e-10)\n",
    "            cumprobs = probs.cumsum()\n",
    "            r = np.random.random()\n",
    "            for j, p in enumerate(cumprobs):\n",
    "                if r < p:\n",
    "                    centroids.append(X[j])\n",
    "                    break\n",
    "        \n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def _assign_clusters(self, X):\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            distances[:, k] = np.sqrt(np.sum((X - self.centroids[k]) ** 2, axis=1))\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        return self._assign_clusters(X)\n",
    "    \n",
    "    def get_cluster_examples(self, cluster_id, df, X_all, top_n=5):\n",
    "        logging.info(f\"Generating examples for cluster {cluster_id}...\")\n",
    "        try:\n",
    "            cluster_labels = self.predict(X_all)\n",
    "            cluster_mask = cluster_labels == cluster_id\n",
    "            cluster_df = df[cluster_mask].copy()\n",
    "            \n",
    "            if len(cluster_df) == 0:\n",
    "                return []\n",
    "            \n",
    "            cluster_df['diversity_score'] = (\n",
    "                cluster_df['Ram'] * 0.5 +\n",
    "                cluster_df['SSD'] * 0.001 +\n",
    "                cluster_df['ppi'] * 0.02 +\n",
    "                (cluster_df['Weight'] * -1) +\n",
    "                cluster_df['Touchscreen'] * 5 +\n",
    "                cluster_df['Ips'] * 5\n",
    "            )\n",
    "            \n",
    "            cluster_df_sorted = cluster_df.sort_values(['diversity_score', 'Price'], \n",
    "                                                      ascending=[False, True])\n",
    "            \n",
    "            examples = []\n",
    "            for _, laptop in cluster_df_sorted.head(top_n).iterrows():\n",
    "                company = laptop.get('Company', 'Unknown')\n",
    "                type_name = laptop.get('TypeName', 'Laptop')\n",
    "                ram = laptop.get('Ram', 0)\n",
    "                ssd = laptop.get('SSD', 0)\n",
    "                hdd = laptop.get('HDD', 0)\n",
    "                cpu = laptop.get('Cpu brand', 'Unknown')\n",
    "                gpu = laptop.get('Gpu brand', 'Unknown')\n",
    "                weight = laptop.get('Weight', 0)\n",
    "                price = laptop.get('Price', 0)\n",
    "                \n",
    "                storage_parts = []\n",
    "                if ssd > 0:\n",
    "                    storage_parts.append(f\"{ssd}GB SSD\")\n",
    "                if hdd > 0:\n",
    "                    storage_parts.append(f\"{hdd}GB HDD\")\n",
    "                storage = \" + \".join(storage_parts) if storage_parts else \"No storage info\"\n",
    "                \n",
    "                features = []\n",
    "                if laptop.get('Touchscreen', 0):\n",
    "                    features.append('Touchscreen')\n",
    "                if laptop.get('Ips', 0):\n",
    "                    features.append('IPS Display')\n",
    "                features_text = ', '.join(features) if features else 'Standard Features'\n",
    "                \n",
    "                example = {\n",
    "                    'Company': company,\n",
    "                    'TypeName': type_name,\n",
    "                    'Title': f\"{company} {type_name}\",\n",
    "                    'Ram': f\"{ram}GB\",\n",
    "                    'Storage': storage,\n",
    "                    'Cpu_brand': cpu,\n",
    "                    'Gpu_brand': gpu,\n",
    "                    'Weight': f\"{weight:.1f}kg\" if weight > 0 else \"Weight N/A\",\n",
    "                    'Price': f\"RS {price:,.2f}\",\n",
    "                    'Features': features_text,\n",
    "                    'Touchscreen': 'Yes' if laptop.get('Touchscreen', 0) else 'No',\n",
    "                    'Ips': 'Yes' if laptop.get('Ips', 0) else 'No',\n",
    "                    'os': laptop.get('os', 'Unknown OS')\n",
    "                }\n",
    "                examples.append(example)\n",
    "            \n",
    "            return examples\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting cluster examples: {e}\")\n",
    "            return []\n",
    "\n",
    "# ====================== DATA PIPELINE ======================\n",
    "\n",
    "logging.info(\"Loading and preprocessing data...\")\n",
    "try:\n",
    "    df = pd.read_csv('laptop_data.csv')\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"laptop_data.csv not found!\")\n",
    "    raise\n",
    "\n",
    "# Data cleaning\n",
    "df[\"Ram\"] = df[\"Ram\"].str.replace(\"GB\", \"\", regex=False).astype(\"int\")\n",
    "df[\"Weight\"] = df[\"Weight\"].str.replace(\"kg\", \"\", regex=False).str.strip()\n",
    "df[\"Weight\"] = pd.to_numeric(df[\"Weight\"], errors='coerce').fillna(df[\"Weight\"].mode()[0]).astype(\"float\")\n",
    "\n",
    "# Handle outliers in Price and Weight\n",
    "df['Price'] = df['Price'].clip(upper=df['Price'].quantile(0.99))\n",
    "df['Weight'] = df['Weight'].clip(upper=df['Weight'].quantile(0.99))\n",
    "\n",
    "# Feature engineering\n",
    "df[\"Touchscreen\"] = df[\"ScreenResolution\"].apply(lambda x: 1 if \"Touchscreen\" in str(x) else 0)\n",
    "df[\"Ips\"] = df[\"ScreenResolution\"].apply(lambda x: 1 if \"IPS\" in str(x) else 0)\n",
    "\n",
    "# Process resolution\n",
    "try:\n",
    "    temp = df[\"ScreenResolution\"].str.split(\"x\", n=1, expand=True)\n",
    "    df[\"X_res\"] = temp[0].str.replace(',', '').str.findall(r'(\\d+\\.?\\d+)').apply(lambda x: x[0] if x else '1920').astype(int)\n",
    "    df[\"Y_res\"] = temp[1].astype(int)\n",
    "    df['ppi'] = (((df['X_res']**2) + (df['Y_res']**2))**0.5/df['Inches']).astype('float')\n",
    "    df.drop(columns=[\"ScreenResolution\", \"X_res\", \"Y_res\", \"Inches\"], inplace=True)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error processing resolution: {e}\")\n",
    "    raise\n",
    "\n",
    "# Process CPU\n",
    "df['Cpu Name'] = df['Cpu'].apply(lambda x: \" \".join(x.split()[0:3]))\n",
    "def fetch_processor(text):\n",
    "    if text in ['Intel Core i7', 'Intel Core i5', 'Intel Core i3']:\n",
    "        return text\n",
    "    elif text.split()[0] == 'Intel':\n",
    "        return 'Other Intel Processor'\n",
    "    else:\n",
    "        return 'AMD Processor'\n",
    "df['Cpu brand'] = df['Cpu Name'].apply(fetch_processor)\n",
    "df.drop(columns=['Cpu', 'Cpu Name'], inplace=True)\n",
    "\n",
    "# Process Memory\n",
    "df['Memory'] = df['Memory'].astype(str).replace(r'\\.0', '', regex=True)\n",
    "df[\"Memory\"] = df[\"Memory\"].str.replace('GB', '', regex=False).str.replace('TB', '000', regex=False)\n",
    "new = df[\"Memory\"].str.split(\"+\", n=1, expand=True)\n",
    "df[\"first\"] = new[0].str.strip().str.replace(r'\\D', '', regex=True).astype(int)\n",
    "df[\"second\"] = new[1].fillna(\"0\").str.replace(r'\\D', '', regex=True).astype(int)\n",
    "df[\"HDD\"] = (df[\"first\"] * df[\"first\"].apply(lambda x: 1 if \"HDD\" in str(x) else 0)) + \\\n",
    "            (df[\"second\"] * df[\"second\"].apply(lambda x: 1 if \"HDD\" in str(x) else 0))\n",
    "df[\"SSD\"] = (df[\"first\"] * df[\"first\"].apply(lambda x: 1 if \"SSD\" in str(x) else 0)) + \\\n",
    "            (df[\"second\"] * df[\"second\"].apply(lambda x: 1 if \"SSD\" in str(x) else 0))\n",
    "df.drop(columns=['first', 'second', 'Memory'], inplace=True)\n",
    "\n",
    "# Process GPU\n",
    "df['Gpu brand'] = df['Gpu'].apply(lambda x: x.split()[0])\n",
    "df = df[df['Gpu brand'] != 'ARM']\n",
    "df.drop(columns=['Gpu'], inplace=True)\n",
    "\n",
    "# Process OS\n",
    "def cat_os(inp):\n",
    "    if inp in ['Windows 10', 'Windows 7', 'Windows 10 S']:\n",
    "        return 'Windows'\n",
    "    elif inp in ['macOS', 'Mac OS X']:\n",
    "        return 'Mac'\n",
    "    else:\n",
    "        return 'Others/No OS/Linux'\n",
    "df['os'] = df['OpSys'].apply(cat_os)\n",
    "df.drop(columns=['OpSys'], inplace=True)\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['Price'])\n",
    "y = np.log(df['Price'])  # Log-transform Price\n",
    "logging.info(f\"Target (y) range: min={y.min():.4f}, max={y.max():.4f}, mean={y.mean():.4f}\")\n",
    "\n",
    "# Preprocessing pipeline for Random Forest and KNN\n",
    "cat_cols = ['Company', 'TypeName', 'Cpu brand', 'Gpu brand', 'os']\n",
    "num_cols = ['Ram', 'Weight', 'Touchscreen', 'Ips', 'ppi', 'HDD', 'SSD']\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for K-Means (numerical features only)\n",
    "kmeans_preprocessor = StandardScaler()\n",
    "X_num = df[num_cols]\n",
    "X_num_transformed = kmeans_preprocessor.fit_transform(X_num)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "X_train_num = kmeans_preprocessor.transform(X_train[num_cols])\n",
    "X_test_num = kmeans_preprocessor.transform(X_test[num_cols])\n",
    "\n",
    "# Apply PCA for K-Means\n",
    "pca = PCA(n_components=5)  # Fixed number of components\n",
    "X_train_pca = pca.fit_transform(X_train_num)\n",
    "X_test_pca = pca.transform(X_test_num)\n",
    "logging.info(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# ====================== MODEL TRAINING ======================\n",
    "\n",
    "# Tune Random Forest\n",
    "logging.info(\"Tuning Random Forest...\")\n",
    "best_rf = None\n",
    "best_r2_rf = -float('inf')\n",
    "rf_params = [(n, d, s) for n in [100, 200] for d in [10, 20, None] for s in [10, 20]]\n",
    "for n_estimators, max_depth, min_samples_split in rf_params:\n",
    "    rf = RandomForest(n_estimators=n_estimators, max_depth=max_depth, max_features='sqrt')\n",
    "    rf.fit(X_train_transformed, y_train)\n",
    "    y_pred = rf.predict(X_test_transformed)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    logging.info(f\"R² for n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}: {r2:.4f}\")\n",
    "    if r2 > best_r2_rf:\n",
    "        best_r2_rf = r2\n",
    "        best_rf = rf\n",
    "\n",
    "rf_model = best_rf\n",
    "y_pred_rf = rf_model.predict(X_test_transformed)\n",
    "logging.info(f\"Random Forest predictions range: min={y_pred_rf.min():.4f}, max={y_pred_rf.max():.4f}, mean={y_pred_rf.mean():.4f}\")\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "logging.info(\"\\nRandom Forest Performance:\")\n",
    "logging.info(f\"MSE: {mse_rf:.4f}\")\n",
    "logging.info(f\"MAE: {mae_rf:.4f}\")\n",
    "logging.info(f\"R² Score: {r2_rf:.4f}\")\n",
    "\n",
    "# Fallback: Train sklearn RandomForestRegressor\n",
    "logging.info(\"\\nTraining sklearn RandomForestRegressor for comparison...\")\n",
    "sklearn_rf = RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_split=10, random_state=42)\n",
    "sklearn_rf.fit(X_train_transformed, y_train)\n",
    "y_pred_sklearn_rf = sklearn_rf.predict(X_test_transformed)\n",
    "mse_sklearn_rf = mean_squared_error(y_test, y_pred_sklearn_rf)\n",
    "mae_sklearn_rf = mean_absolute_error(y_test, y_pred_sklearn_rf)\n",
    "r2_sklearn_rf = r2_score(y_test, y_pred_sklearn_rf)\n",
    "\n",
    "logging.info(\"\\nSklearn RandomForestRegressor Performance:\")\n",
    "logging.info(f\"MSE: {mse_sklearn_rf:.4f}\")\n",
    "logging.info(f\"MAE: {mae_sklearn_rf:.4f}\")\n",
    "logging.info(f\"R² Score: {r2_sklearn_rf:.4f}\")\n",
    "\n",
    "# Train KNN\n",
    "logging.info(\"\\nTraining KNN...\")\n",
    "knn_model = CustomKNN(k=5, metric='cosine')\n",
    "knn_model.fit(X_train_transformed, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test_transformed)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "logging.info(\"\\nKNN Performance:\")\n",
    "logging.info(f\"MSE: {mse_knn:.4f}\")\n",
    "logging.info(f\"MAE: {mae_knn:.4f}\")\n",
    "logging.info(f\"R² Score: {r2_knn:.4f}\")\n",
    "\n",
    "# Tune K-Means with Elbow Method\n",
    "logging.info(\"\\nTuning K-Means...\")\n",
    "best_kmeans = None\n",
    "best_silhouette = -float('inf')\n",
    "inertia = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = CustomKMeans(n_clusters=k, max_iters=100, random_state=42, init='k-means++')\n",
    "    kmeans.fit(X_train_pca)\n",
    "    score = silhouette_score(X_train_pca, kmeans.labels_)\n",
    "    inertia.append(np.sum([np.sum((X_train_pca[kmeans.labels_ == i] - kmeans.centroids[i])**2) \n",
    "                           for i in range(k)]))\n",
    "    logging.info(f\"Silhouette Score for k={k}: {score:.4f}\")\n",
    "    if score > best_silhouette:\n",
    "        best_silhouette = score\n",
    "        best_kmeans = kmeans\n",
    "\n",
    "kmeans_model = best_kmeans\n",
    "silhouette = silhouette_score(X_train_pca, kmeans_model.labels_)\n",
    "logging.info(\"\\nK-Means Performance:\")\n",
    "logging.info(f\"Best Number of Clusters: {kmeans_model.n_clusters}\")\n",
    "logging.info(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "logging.info(f\"Inertia for k={kmeans_model.n_clusters}: {inertia[kmeans_model.n_clusters-2]:.4f}\")\n",
    "\n",
    "# ====================== SAVE MODELS ======================\n",
    "\n",
    "logging.info(\"\\nSaving models...\")\n",
    "joblib.dump({\n",
    "    'df': df,\n",
    "    'preprocessor': preprocessor,\n",
    "    'kmeans_preprocessor': kmeans_preprocessor,\n",
    "    'pca': pca,\n",
    "    'random_forest': rf_model,\n",
    "    'sklearn_random_forest': sklearn_rf,\n",
    "    'knn': knn_model,\n",
    "    'kmeans': kmeans_model\n",
    "}, 'laptop_models_improved.pkl')\n",
    "\n",
    "logging.info(\"Saved successfully to laptop_models_improved.pkl ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c3887-d961-484a-8b65-6f437f7db49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
